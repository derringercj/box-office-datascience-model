# -*- coding: utf-8 -*-
"""CS551FinalProject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18IcM6kZYJgfq83sleA-MdJzv07WK6tLK
"""

import kagglehub

# Download latest version
path = kagglehub.dataset_download("alanvourch/tmdb-movies-daily-updates")

print("Path to dataset files:", path)

import pandas as pd

movies_df = pd.read_csv(path+"/TMDB_all_movies.csv")
columns_to_drop = [
    "id",
    "vote_average",
    "vote_count",
    "imdb_id",
    "poster_path",
    "imdb_rating",
    "imdb_votes",
    "tagline",
    "production_countries",
    "producers",
    "popularity",
    "overview",
    "original_title"
]
movies_df.drop(labels=columns_to_drop, axis=1, inplace=True)

# Cleaning empty titles
empty_titles = movies_df["title"].isna()
movies_df = movies_df[~empty_titles]

# Cleaning unreleased movies
unreleased = (movies_df["status"] != "Released")
movies_df = movies_df[~unreleased]
movies_df.drop(columns=["status"], inplace=True)

# One-hot encoding genres
movies_df["genres"] = movies_df["genres"].fillna("").apply(
    lambda string: string.split(", ")
)
unique_genres = set()
for genres in movies_df["genres"]:
  unique_genres.update(genres)
unique_genres.remove("")
for genre in unique_genres:
  movies_df[genre] = movies_df["genres"].apply(
    lambda movie_genres: 1 if genre in movie_genres else 0
  )
movies_df.drop(columns=["genres"], inplace=True)


# removing movies with 0 revenue
streaming = (movies_df["revenue"] == 0)
movies_df = movies_df[~streaming]

# removing prior to 2012 ?
old = (movies_df["release_date"] < "2012-01-01")
movies_df = movies_df[~old]

# converting to datetime
release_dates = pd.to_datetime(movies_df["release_date"])
movies_df["release_date"] = release_dates

# Removing movies without a release date
no_date = (movies_df["release_date"].isna())
movies_df = movies_df[~no_date]

# Creating boolean column showing whether a movie is spoken in english or not
movies_df["in_english"]= movies_df["spoken_languages"].apply(lambda x: "English" in str(x))
movies_df.drop(columns=["original_language", "spoken_languages"], inplace=True)

# Creating release year and release month columns that are interpretable by the model
movies_df["release_year"] = movies_df["release_date"].dt.year
movies_df["release_month"] = movies_df["release_date"].dt.month

# Engineering is_peak_season feature
def is_peak_season(release_month):
  if release_month >= 5 and release_month <= 8:
    return True
  elif release_month >= 11:
    return True
  else:
    return False
movies_df["is_peak_season"] = movies_df["release_month"].apply(is_peak_season)

# Sequel Detection
import re # main method is with regular expressions for common terms

def detect_sequel(title): # binary encoding of sequel or not, based on numbers roman numerals and other common phrases
    # pasted in common movie phrases: NOT PERFECT, CAN GET FALSE POSITIVES
    patterns = r"(?:\b(II|III|IV|V|VI|VII|VIII|IX|X|XI|XII|XIII|XIV|XV|2|3|4|5|6|7|8|9|10|11|12|13|14|15)\b|Part \d+|Chapter \d+|:\s*The\s+(Sequel|Return|Revenge|Awakens|End|Beginning|Rise|Fall|Origins|Legacy|Reloaded|Resurrection|Reborn|Revolution|Reload|Remake|Reboot|Returns|Strikes Back|Awakening|Final|Ultimate|Extended|Director\'s Cut)|Sequel|Returns|Revenge|Awakens)"
    return 1 if re.search(patterns, title, re.IGNORECASE) else 0

movies_df["is_sequel"] = movies_df["title"].apply(detect_sequel) # adds a new column to the full dataset with binary encoded data: 1 : sequel :: 0 : no sequel

# split our various categorical variables into lists
movies_df["director"] = movies_df["director"].fillna("").apply(
    lambda x: [d.strip() for d in str(x).split(",") if d.strip()]
)
movies_df["writers"] = movies_df["writers"].fillna("").apply(
    lambda x: [d.strip() for d in str(x).split(",") if d.strip()]
)
movies_df["production_companies"] = movies_df["production_companies"].fillna("").apply(
    lambda x: [d.strip() for d in str(x).split(",") if d.strip()]
)
movies_df["music_composer"] = movies_df["music_composer"].fillna("").apply(
    lambda x: [d.strip() for d in str(x).split(",") if d.strip()]
)
movies_df["cast"] = movies_df["cast"].fillna("").apply(
    lambda x: [d.strip() for d in str(x).split(",") if d.strip()]
)

def loo_encoding_with_regularization(m, feature, movies_df, mechanism):
  exploded = movies_df[[feature, "revenue"]].explode(feature).copy()

  global_mean_rev = exploded["revenue"].mean()

  stats = exploded.groupby(feature)["revenue"].agg(["count", "mean", "sum"])

  exploded["sum"] = exploded[feature].map(stats["sum"])
  exploded["mean"] = exploded[feature].map(stats["mean"])
  exploded["count"] = exploded[feature].map(stats["count"])

  exploded["loo_sum"] = exploded["sum"] - exploded["revenue"]
  exploded["loo_count"] = exploded["count"] - 1


  exploded[f"{feature}_target"] = (
    (exploded["loo_sum"] + m * global_mean_rev)
    / (exploded["loo_count"] + m)
  )

  if mechanism == "mean":
    movies_df[f"{feature}_score"] = exploded.groupby(exploded.index)[f"{feature}_target"].mean()
  else:
    movies_df[f"{feature}_score"] = exploded.groupby(exploded.index)[f"{feature}_target"].max()

loo_encoding_with_regularization(5, "director", movies_df, "mean")
loo_encoding_with_regularization(10, "writers", movies_df, "mean")
loo_encoding_with_regularization(20, "production_companies", movies_df, "max")

# Encoding casts
exploded = movies_df[["cast", "revenue"]].explode("cast")

global_mean_rev = exploded["revenue"].mean()

stats = exploded.groupby("cast")["revenue"].agg(["count", "mean", "sum"])

exploded["sum"] = exploded["cast"].map(stats["sum"])
exploded["mean"] = exploded["cast"].map(stats["mean"])
exploded["count"] = exploded["cast"].map(stats["count"])

exploded["loo_sum"] = exploded["sum"] - exploded["revenue"]
exploded["loo_count"] = exploded["count"] - 1

m = 15
exploded["actor_target"] = (
  (exploded["loo_sum"] + m * global_mean_rev)
  / (exploded["loo_count"] + m)
)

exploded = exploded.sort_values("actor_target", ascending=False)
top_k_actors = exploded.groupby(level=0).head(5)

cast_score = top_k_actors.groupby(level=0)["actor_target"].mean()
cast_score = cast_score.fillna(global_mean_rev)

movies_df["cast_target"] = cast_score

# Director of Photography experience encoding
dop_map = movies_df["director_of_photography"].value_counts()
movies_df["dop_experience"] = movies_df["director_of_photography"].map(dop_map)

# Binary Encoding the top 60 composers, accounting for about 10% of the data
exploded = movies_df["music_composer"].explode()
counts = exploded.value_counts()

top_composers = counts.head(60).index.tolist()
def encode_composers(composers):
  for composer in composers:
    if composer in top_composers:
      return True
  return False

movies_df["is_top_composer"] = movies_df["music_composer"].apply(encode_composers)

import numpy as np
# Drive link to the scraped_ewom_data.json zip file
# https://drive.google.com/file/d/1I4kBNq6uEykz4GS1aAMhjwsYfbvI9Ei9/view?usp=sharing

# Loading the data pulled from the scraper with youtube trailer view counts and like counts
scraped_data = pd.read_json("scraped_ewom_data.json")
scraped_data = scraped_data.rename(columns={"input": "query"})

# Adding the search queries to the dataframe to be joined upon with the scraped data later
movies_df["query"] = (movies_df["title"] + " " + movies_df["release_date"].dt.year.astype(str) + " trailer")

# Merging the scraped data with our movies dataframe
movies_df = movies_df.merge(scraped_data, on="query", how="left")
movies_df.rename(columns={"viewCount":"trailer_view_count", "likes":"trailer_likes"}, inplace=True)
movies_df.drop(columns=["query"], inplace=True)

# Creating a separate movies dataframe featuring exclusively movies that have accompanying ewom metrics
no_youtube_mask = movies_df["trailer_view_count"].isna()
movies_df_with_yt = movies_df[~no_youtube_mask]

# Column Selection
numberTypes = ["int64", "float64", "bool"]
feature_columns = [col for col in movies_df.columns if movies_df[col].dtype.name in numberTypes and col != "revenue"]

# Creating separate list of features without youtube metrics to be used with full movies df
feature_columns_noscraped = feature_columns.copy()
feature_columns_noscraped.remove("trailer_view_count")
feature_columns_noscraped.remove("trailer_likes")

print(feature_columns)

from sklearn.model_selection import RepeatedKFold, cross_validate
from sklearn.ensemble import RandomForestRegressor
from sklearn.tree import DecisionTreeRegressor
import matplotlib.pyplot as plt
import statistics

# Defining function for training and evaluating our tree models
def model_training_and_evaluation(X, y, columns):
  tree_model = DecisionTreeRegressor(max_depth=10, random_state=42)

  repeated_kf = RepeatedKFold(n_splits=10, n_repeats=1, random_state=42)
  tree_results = cross_validate(
      tree_model, X, y, scoring=["r2", "neg_mean_absolute_error"], cv=repeated_kf

  )
  print(f"Median and Mean R^2 Value: {statistics.median(tree_results["test_r2"])} - {statistics.mean(tree_results["test_r2"])}")
  print(f"Median and Mean MAE: {statistics.median(tree_results["test_neg_mean_absolute_error"])} - {statistics.mean(tree_results["test_neg_mean_absolute_error"])}")

  tree_model.fit(X, y)
  # Printing feature importances
  importances = pd.DataFrame({"feature": columns, "importance": tree_model.feature_importances_}).sort_values("importance", ascending=False)
  print("Decision Tree Feature Importances:")
  print(importances)

  # Random Forest model
  randomforest_model = RandomForestRegressor(n_estimators=100, max_depth=8, min_samples_leaf=10, random_state=42)

  forest_results = cross_validate(
      randomforest_model, X, y, scoring=["r2", "neg_mean_absolute_error"], cv=repeated_kf

  )
  print(f"Median  and Mean R^2 Value: {statistics.median(forest_results["test_r2"])} - {statistics.mean(forest_results["test_r2"])}")
  print(f"Median and Mean MAE: {statistics.median(forest_results["test_neg_mean_absolute_error"])} - {statistics.mean(forest_results["test_neg_mean_absolute_error"])}")

  randomforest_model.fit(X, y)
  # Printing Feature Importances
  importances = pd.DataFrame({"feature": columns, "importance": randomforest_model.feature_importances_}).sort_values("importance", ascending=False)
  print("Random Forest Feature Importances:")
  print(importances)

  return randomforest_model

# Spliting of the data
X = movies_df[feature_columns_noscraped]
y = movies_df["revenue"]

print("Training and evaluating non-youtube model")
rf_no_youtube = model_training_and_evaluation(X, y, feature_columns_noscraped)

# Re-spliting of the data to include youtube metrics
X = movies_df_with_yt[feature_columns]
y = movies_df_with_yt["revenue"]
print("Training and evaluating youtube model")
rf_w_youtube = model_training_and_evaluation(X, y, feature_columns)

def encode(entry, feature):
  # Looking up the mean revenue values from our training data, then either averaging them or returning the max over all entries given, giving us the score
  exploded = movies_df[[feature, "revenue"]].explode(feature).copy()

  global_mean_rev = exploded["revenue"].mean()
  stats = exploded.groupby(feature)["revenue"].agg(["mean"])
  # Creating a lookup map for our feature and their mean targets
  encoding_dictionary = stats["mean"].to_dict()

  if feature == "director":
    score = encoding_dictionary.get(entry, global_mean_rev)
  elif feature == "writers" or feature == "cast":
    cumulative_scores = [encoding_dictionary.get(item, global_mean_rev) for item in entry]
    score = statistics.mean(cumulative_scores)
  elif feature == "production_companies":
    cumulative_scores = [encoding_dictionary.get(item, global_mean_rev) for item in entry]
    score = max(cumulative_scores)

  return score

def compute_results(director, writers, prod_companies, cast, budget, genres, runtime, dop_exp, release_date, checkboxes, composer, views, likes):
  # Setting the appropriate features based on what we received from the checkboxes
  if "Part of an Existing Franchise" in checkboxes:
    is_sequel=True
  else:
    is_sequel = False
  if "Primary Language English" in checkboxes:
    english = True
  else:
    english = False

  # Returning if the movie releases in peak season with our previously defined is_peak_season function
  if release_date == None:
    raise gr.Error("Please enter a release date!")
  season = is_peak_season(release_date.month)

  # Check if the entered composer is in the above list of "top" composers
  if composer == "":
    top_composer = False
  else:
    top_composer = composer in top_composers

  # Computing the director score via the encode function
  director_score = encode(director, "director")

  # Splitting the comma separated writers, companies, and cast entries into a list, then generating their score via the encode function
  writers_list = writers.split(",")
  writers_score = encode(writers_list, "writers")
  production_companies_list = prod_companies.split(",")
  production_companies_score = encode(production_companies_list, "production_companies")
  cast_list = cast.split(",")
  cast_score = encode(cast_list, "cast")

  # Creating a dictionary representing one hot encoded genres for the user's entry
  genre_flags = {}
  possible_genres = list(unique_genres)
  for genre in possible_genres:
    if genre in genres:
      genre_flags[genre] = 1
    else:
      genre_flags[genre] = 0
  # Creating a dictionary of our sample data to make a dataframe out of
  sample_data = {
      "director_score": director_score,
      "writers_score": writers_score,
      "production_companies_score": production_companies_score,
      "cast_target": cast_score,
      "budget": int(budget),
      "runtime": int(runtime),
      "dop_experience": dop_exp,
      "is_peak_season": season,
      "is_top_composer": top_composer,
      "in_english": english,
      "is_sequel": is_sequel,
  }
  # Adding the one hot encoded genres to our user sample
  for genre, value in genre_flags.items():
      sample_data[genre] = value

  # If we are given youtube data (views and likes), add them to the sample and run our prediction with the forest trained on the youtube data
  if views != "":
    sample = pd.DataFrame([sample_data])

    sample["trailer_view_count"] = int(views)
    sample["trailer_likes"] = int(likes)

    # Reordering sample columns to match when the model was fit
    sample = sample[feature_columns]

    estimate = rf_w_youtube.predict(sample)[0]

    opening_weekend = round(estimate * 0.30, -3)
  # Otherwise run the sample prediction on our no youtube model
  else:
    sample = pd.DataFrame([sample_data])
    for genre, value in genre_flags.items():
      sample[genre] = value

    # Reordering sample columns to match when the model was fit
    sample = sample[feature_columns_noscraped]

    estimate = rf_no_youtube.predict(sample)[0]

    opening_weekend = round(estimate * 0.30, -3)

  return f"{round(estimate, -3):,} ({opening_weekend:,} opening weekend)"

# User Interface Implementation
import gradio as gr

with gr.Blocks() as demo:
  with gr.Row():
    with gr.Column():
      director = gr.Textbox(label="Director", placeholder="Director Name (one value only)")
      writers = gr.Textbox(label="Writers", placeholder="Writer Names (if multiple, comma separated list)")
      prod_company = gr.Textbox(label="Production Companies", placeholder="Company Name (if multiple, comma separated list)")
      cast = gr.Textbox(label="Cast", placeholder="Actors in the movie (comma separated list, 5 entries)")
      budget = gr.Textbox(label="Budget", placeholder="Production Budget (integer)")
      genres = gr.Dropdown(choices=list(unique_genres), type="value", multiselect=True, label="Movie Genre(s)")
    with gr.Column():
      runtime = gr.Slider(minimum=1, maximum=240, step=5, label="Movie Runtime (in minutes)", interactive=True)
      dop_exp = gr.Slider(minimum=0, maximum=30, step=1, label="Director of Photography Experience (# of past projects)", interactive=True)
      release_date = gr.DateTime(include_time=False, type="datetime", label="Projected Release Date")
      checkboxes = gr.CheckboxGroup(choices=["Primary Language English", "Part of an Existing Franchise"], label="Check all that apply:" )
      composer = gr.Textbox(label="Music Composer", placeholder="Composer Name (one value only)")
      with gr.Accordion(label="Youtube Metrics (if available)", open=False):
        views = gr.Textbox(label="Youtube Trailer Views", placeholder="View Count")
        likes = gr.Textbox(label="Youtube Trailer Likes", placeholder="Like Count")
  with gr.Row():
    generate = gr.Button(value="Run")
  with gr.Column():
    estimate = gr.Textbox(label="Estimate")
    # opening_weekend = gr.Textbox(label="Projected Opening Weekend", value=)

  generate.click(compute_results, inputs=[director, writers, prod_company, cast, budget, genres, runtime, dop_exp, release_date, checkboxes, composer, views, likes], outputs=estimate)

demo.launch(share=True)